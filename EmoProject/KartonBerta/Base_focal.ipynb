{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e47a69-6ae8-4274-ae04-0565fe2c5af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import project_functions as pf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from transformers import (AutoTokenizer,\n",
    "                          BertForSequenceClassification,\n",
    "                          RobertaForSequenceClassification,\n",
    "                          get_linear_schedule_with_warmup)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab7503-9049-42b5-8139-1c8d6ed9bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, encodings, labels):\n",
    " \n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_labels(self):   \n",
    "        \n",
    "        return self.labels\n",
    "\n",
    "\n",
    "# Defining focal loss to use during training\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha (tensor or float): tensor of shape (num_labels,) with a separate alpha for each label, \n",
    "                                     or float for a constant alpha across labels.\n",
    "            gamma (float): focusing parameter.\n",
    "            reduction (str): 'none' | 'mean' | 'sum'.\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha  \n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            self.alpha = self.alpha.to(inputs.device)\n",
    "            \n",
    "        \n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * BCE_loss\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            focal_loss = self.alpha * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb5d26e-df79-429c-be4b-2384761b751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "train_val_data = pd.read_csv('train_val.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ffe5c8-2977-4dc3-998d-7e7a0a100cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting reviews into sentences and whole reviews - train set\n",
    "sentences_train = pf.transform_sentences(train_val_data)\n",
    "texts_train = pf.transform_text(train_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9445ad-a53f-4736-af14-ad4a3ae07ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting reviews into sentences and whole reviews - test set\n",
    "sentences_test = pf.transform_sentences(test_data)\n",
    "texts_test = pf.transform_text(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3921c44b-c459-4b1d-bf6e-e557021b0435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining alpha weights as inverted frequency of labels, setting gamma parameter\n",
    "freqs = sentences_train.iloc[:,1:].to_numpy().sum(axis=0)\n",
    "num = len(sentences_train)\n",
    "\n",
    "inv_freq = num / freqs\n",
    "\n",
    "alpha_weights = torch.tensor(inv_freq)\n",
    "gamma_val = torch.tensor(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c987deb-23ae-4f56-a411-10e3bdecf7e9",
   "metadata": {},
   "source": [
    "### Defining model and model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66851ce3-c157-49e4-9243-afd8b7fc5b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining model parameters and seed for comparison purposes\n",
    "model_path = 'OrlikB/KartonBERT-USE-base-v1'\n",
    "ModelClass = BertForSequenceClassification\n",
    "\n",
    "use_gpu_if_available = True\n",
    "batch_size_train = 8\n",
    "batch_size_eval = 8\n",
    "max_tokenizer_length = 512\n",
    "\n",
    "num_epochs = 8\n",
    "warming_steps = 100\n",
    "lr = 2e-5\n",
    "weight_decay = 0.01\n",
    "eval_steps_per_epoch = 1 \n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f6a74a-708f-43d1-98dd-bf9b84b1da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up device for training and testing\n",
    "device = 'cuda' if torch.cuda.is_available() and use_gpu_if_available else 'cpu'\n",
    "print(f'Training device set to: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7124a0-90e1-465f-92f4-5b52c71ba006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transferring parameters to device\n",
    "gamma_val.to(device)\n",
    "alpha_weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c4f9c-a334-420f-9a95-8c840f1cc733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the number of labels for the multilabel task\n",
    "classes = sentences_train.columns[1:]\n",
    "labels = sentences_train.columns[1:]\n",
    "labels = [s.strip() for s in labels]\n",
    "\n",
    "NUM_LABELS = len(labels)\n",
    "id2label = {idx: label for idx, label in enumerate(sorted(labels))}\n",
    "label2id = {label: idx for idx, label in enumerate(sorted(labels))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55249f33-b1e8-4b90-8619-35bbd4e032a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, max_length=max_tokenizer_length)\n",
    "model = ModelClass.from_pretrained(model_path, num_labels=NUM_LABELS, id2label=id2label, label2id=label2id)\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180cf8ab-3eac-4f52-a11c-d72b4c6321cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data for training \n",
    "train_texts = list(sentences_train['text'])\n",
    "test_texts = list(sentences_test['text'])\n",
    "\n",
    "train_labels = sentences_train.iloc[:,1:].astype(int).to_numpy()\n",
    "test_labels = sentences_test.iloc[:,1:].astype(int).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80942022-716f-479d-83d3-cd27cce9c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_tokenizer_length)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_tokenizer_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d219654-0ee1-4011-834f-89245239dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_encodings, train_labels)\n",
    "test_dataset = TextDataset(test_encodings, test_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size_eval, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f75eee-56da-4ab5-9dbd-1d4c946f52b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model setup\n",
    "eval_step_list = np.linspace(0, len(train_dataloader), eval_steps_per_epoch+1).astype(int).tolist()[1:]\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warming_steps, num_training_steps=total_steps)\n",
    "criterion = FocalLoss(alpha=alpha_weights, gamma=gamma_val, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6134a-328e-43b9-afd7-dd0aa3df6e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0.0\n",
    "    loss_step_counter = 0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        labels = labels.float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss_step_counter += 1\n",
    "\n",
    "        if (step + 1) in eval_step_list:\n",
    "            eval_time = time.perf_counter()\n",
    "\n",
    "            eval_time = time.perf_counter() - eval_time\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Step [{step + 1}/{len(train_dataloader)}], '\n",
    "                  f'Train Loss: {total_train_loss / loss_step_counter:.2f}, '\n",
    "                  f'lr: {optimizer.param_groups[0][\"lr\"]:.3e}, '\n",
    "                  f'Eval Time: {eval_time:.2f}')\n",
    "\n",
    "            total_train_loss = 0.0\n",
    "            loss_step_counter = 0\n",
    "\n",
    "# saving the model\n",
    "# model.save_pretrained(f'{path}focal_{epoch}')\n",
    "# tokenizer.save_pretrained(f'{path}focal_{epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1232094-1dc1-4d69-a772-8e135d221052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting raw data from model\n",
    "labels_test, predictions_test = pf.get_test_data(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3696c050-1564-41a4-8b51-4165ebe96dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting predictions based on predefined split value\n",
    "split_val = 0.35\n",
    "\n",
    "labels_test = np.array(labels_test)\n",
    "predictions_test = np.array(predictions_test)\n",
    "preds_test = (predictions_test > split_val).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643545c9-9f7b-4aa3-b57a-bb3e042650e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final evaluation\n",
    "test_dict = pf.get_test_evaluation(labels_test, preds_test, classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
